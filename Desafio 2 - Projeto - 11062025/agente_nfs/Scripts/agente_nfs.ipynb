{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3458f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27212,
     "status": "ok",
     "timestamp": 1750214986132,
     "user": {
      "displayName": "Antonio Dantas",
      "userId": "09143204756506017123"
     },
     "user_tz": 180
    },
    "id": "6a3458f4",
    "outputId": "4c17ffe7-1c6b-4c45-ac52-a3413f2d141c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.3.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: pydantic==2.11.7 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 2)) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv==1.1.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: sqlalchemy==2.0.41 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.41)\n",
      "Collecting packaging==24.2 (from -r requirements.txt (line 5))\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: protobuf<6.31.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 6)) (5.29.5)\n",
      "Collecting langchain==0.3.25 (from -r requirements.txt (line 7))\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core==0.3.65 (from -r requirements.txt (line 8))\n",
      "  Using cached langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting google-generativeai==0.8.5 (from -r requirements.txt (line 9))\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from -r requirements.txt (line 10))\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain-google-genai==2.0.10 (from -r requirements.txt (line 11))\n",
      "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: streamlit==1.46.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from -r requirements.txt (line 12)) (1.46.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pandas==2.3.0->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pandas==2.3.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pandas==2.3.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pandas==2.3.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pydantic==2.11.7->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pydantic==2.11.7->-r requirements.txt (line 2)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pydantic==2.11.7->-r requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pydantic==2.11.7->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from sqlalchemy==2.0.41->-r requirements.txt (line 4)) (3.2.3)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from langchain==0.3.25->-r requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from langchain==0.3.25->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from langchain-core==0.3.65->-r requirements.txt (line 8)) (9.1.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core==0.3.65->-r requirements.txt (line 8))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-core (from google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached google_api_python_client-2.174.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: tqdm in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from google-generativeai==0.8.5->-r requirements.txt (line 9)) (4.67.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->-r requirements.txt (line 10))\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from langchain-google-genai==2.0.10->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (6.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (8.2.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (11.2.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (20.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from streamlit==1.46.0->-r requirements.txt (line 12)) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (1.44.0)\n",
      "Requirement already satisfied: colorama in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from click<9,>=7.0->streamlit==1.46.0->-r requirements.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.46.0->-r requirements.txt (line 12)) (4.0.12)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->-r requirements.txt (line 10))\n",
      "  Using cached grpcio-1.73.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->-r requirements.txt (line 10))\n",
      "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit==1.46.0->-r requirements.txt (line 12))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai==0.8.5->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai==0.8.5->-r requirements.txt (line 9)) (4.9.1)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core==0.3.65->-r requirements.txt (line 8))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7)) (0.23.0)\n",
      "Requirement already satisfied: six>=1.5 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.3.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from requests<3,>=2->langchain==0.3.25->-r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from requests<3,>=2->langchain==0.3.25->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from requests<3,>=2->langchain==0.3.25->-r requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from requests<3,>=2->langchain==0.3.25->-r requirements.txt (line 7)) (2025.6.15)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai==0.8.5->-r requirements.txt (line 9))\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from google-api-python-client->google-generativeai==0.8.5->-r requirements.txt (line 9)) (4.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.46.0->-r requirements.txt (line 12)) (5.0.2)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->-r requirements.txt (line 10))\n",
      "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai==0.8.5->-r requirements.txt (line 9)) (3.2.3)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.46.0->-r requirements.txt (line 12)) (0.25.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.5->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in g:\\meu drive\\cursos e treinamentos\\cientista de dados\\treinamento python\\i2a2\\desafios\\desafio 2 - projeto - 11062025\\agente_nfs\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r requirements.txt (line 7)) (1.3.1)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
      "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached google_api_python_client-2.174.0-py3-none-any.whl (13.7 MB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio-1.73.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: proto-plus, packaging, orjson, jsonpointer, httplib2, h11, grpcio, googleapis-common-protos, cachetools, anyio, requests-toolbelt, jsonpatch, httpcore, grpcio-status, google-auth, httpx, google-auth-httplib2, google-api-core, langsmith, google-api-python-client, langchain-core, google-ai-generativelanguage, langchain-text-splitters, google-generativeai, langchain-google-genai, langchain\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 6.1.0\n",
      "    Uninstalling cachetools-6.1.0:\n",
      "      Successfully uninstalled cachetools-6.1.0\n",
      "Successfully installed anyio-4.9.0 cachetools-6.1.0 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.174.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.73.1 grpcio-status-1.71.0 h11-0.16.0 httpcore-1.0.9 httplib2-0.22.0 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.65 langchain-google-genai-2.0.10 langchain-text-splitters-0.3.8 langsmith-0.3.45 orjson-3.10.18 packaging-24.2 proto-plus-1.26.1 requests-toolbelt-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b23614",
   "metadata": {
    "id": "49b23614"
   },
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5916e",
   "metadata": {
    "id": "d3f5916e"
   },
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from io import BytesIO\n",
    "from os.path import exists\n",
    "from re import search\n",
    "from pandas import read_csv, read_sql\n",
    "import sqlalchemy as sqlalc\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class SemArquivoCabecalho(Exception):\n",
    "    pass\n",
    "\n",
    "class SemArquivoItens(Exception):\n",
    "    pass\n",
    "\n",
    "class SemArquivoZip(Exception):\n",
    "    pass\n",
    "\n",
    "class SemResposta(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa6074",
   "metadata": {
    "id": "67aa6074"
   },
   "source": [
    "#### <b>AGENTE 1: Aquisição de Documentos</b>\n",
    "<b>Responsabilidade:</b> Obter e pré-processar documentos fiscais<br/><br/>\n",
    "<b>Funcionalidades:</b>\n",
    "<ul><li>Interface para upload manual de arquivos (PDF, imagens)</li></ul>\n",
    "<ul><li>Integração com APIs de órgãos governamentais (SEFAZ)</li></ul>\n",
    "<ul><li>Validação inicial de formato e integridade dos documentos</li></ul>\n",
    "<ul><li>Organização e catalogação dos arquivos recebidos (Armazenando em banco de dados, se os arquivos responderem a pergunta)</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16247c",
   "metadata": {
    "id": "6b16247c"
   },
   "outputs": [],
   "source": [
    "def agente1(pergunta,engine, arquivos,llm):\n",
    "\n",
    "    print('\\nExecutando agente 1...')\n",
    "\n",
    "    # VALIDAÇÃO DE INTEGRIDADE -> IMPLEMENTAR PARA DETERMINAR SE O ARQUIVO REALMENTE É UM TIPO ZIP. NÃO FAZER PELA EXTENSÃO\n",
    "    # FAZER\n",
    "\n",
    "    \"\"\"\n",
    "        Utilizando a LLM para identificar se os campos e registros da base de documentos, sãa capazes de responder a pergunta\n",
    "        do usuário.\n",
    "\n",
    "        Se sim, os arquivos são persistidos no banco de dados, caso contrário, o arquivo é descartado.\n",
    "    \"\"\"\n",
    "    # FORMATANDO A SAÍDA DA LLM COM JsonOutputParser\n",
    "    class Resposta(BaseModel):\n",
    "        resposta: str = Field(description=\"Responda Sim ou Não\")\n",
    "\n",
    "    parseador = JsonOutputParser(pydantic_object=Resposta)\n",
    "\n",
    "    # CRIANDO O PROMPT PARA A LLM COM A SAIDA FORMATADA\n",
    "    template = \"\"\"É possível responder a pergunta {pergunta} do usuário baseado no dataframe {df} ? {resposta}\"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "                                        template=template,\n",
    "                                        input_variables=[\"pergunta\",\"df\"],\n",
    "                                        partial_variables={\"resposta\" : parseador.get_format_instructions()}\n",
    "                                    )\n",
    "\n",
    "    # CRIANDO A CADEIA DE EXECUÇÃO PARA A LLM\n",
    "    chain = prompt_template | llm | parseador\n",
    "\n",
    "    # CATALOGANDO OS ARQUIVOS NO BD\n",
    "    j=0\n",
    "\n",
    "    inspector = sqlalc.inspect(engine) # INSPECTOR PARA LISTAR AS TABELAS DO BANCO DE DADOS\n",
    "\n",
    "    for f in arquivos:\n",
    "\n",
    "        # SERÁ CRIADO UM DATAFRAME PARA CADA ARQUIVO\n",
    "        if f.get('cabecalho') is not None:\n",
    "            dfcabecalho = read_csv(f.get('cabecalho'))\n",
    "\n",
    "            # INSERINDO COLUNA COM O NOME DO ARQUIVO NO DATAFRAME\n",
    "            dfcabecalho['ARQUIVO'] = f.get('nome_arquivo')\n",
    "            df = dfcabecalho\n",
    "\n",
    "            #print('Dataframe de cabeçalho: ',df)\n",
    "\n",
    "            # INVOCANDO A LLM\n",
    "            resposta = chain.invoke(input={\"pergunta\":pergunta, \"df\": df})['resposta']\n",
    "\n",
    "            if resposta == 'Sim':\n",
    "                j+=1\n",
    "\n",
    "                print('Sim para o arquivo: ',f.get('nome_arquivo'))\n",
    "\n",
    "                # PRECISA VERIFICAR SE A TABELA JÁ EXISTE NO BANCO DE DADOS ANTES DE LER\n",
    "                if 'NFCABECALHO' in inspector.get_table_names():\n",
    "                    dftable = read_sql('NFCABECALHO', con=engine)\n",
    "\n",
    "                    #print('dftable NFCABECALHO\\n',dftable)\n",
    "\n",
    "                    # CUIDANDO DE DUPLICIDADE\n",
    "                    df = df[~df['CHAVE DE ACESSO'].isin(dftable['CHAVE DE ACESSO'])]\n",
    "\n",
    "                # INSERINDO NO BANCO DE DADOS\n",
    "                df.to_sql(name='NFCABECALHO', con=engine, if_exists='append', index=False)\n",
    "\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if f.get('itens') is not None:\n",
    "            dfitens = read_csv(f.get('itens'))\n",
    "\n",
    "            # INSERINDO COLUNA COM O NOME DO ARQUIVO NO DATAFRAME\n",
    "            dfitens['ARQUIVO'] = f.get('nome_arquivo')\n",
    "            df = dfitens\n",
    "\n",
    "            #print('Dataframe de itens: ',df)\n",
    "\n",
    "            # INVOCANDO A LLM\n",
    "            resposta = chain.invoke(input={\"pergunta\":pergunta, \"df\": df})['resposta']\n",
    "\n",
    "            if resposta == 'Sim':\n",
    "                j+=1\n",
    "\n",
    "                print('Sim para o arquivo: ',f.get('nome_arquivo'))\n",
    "\n",
    "                 # PRECISA VERIFICAR SE A TABELA JÁ EXISTE NO BANCO DE DADOS ANTES DE LER\n",
    "                if 'NFITENS' in inspector.get_table_names():\n",
    "                    dftable = read_sql('NFITENS', con=engine)\n",
    "\n",
    "                    #print('dftable NFINTENS\\n',dftable)\n",
    "\n",
    "                    # CUIDANDO DE DUPLICIDADE\n",
    "                    df = df[~df['CHAVE DE ACESSO'].isin(dftable['CHAVE DE ACESSO'])]\n",
    "\n",
    "                # INSERINDO NO BANCO DE DADOS\n",
    "                df.to_sql(name='NFITENS', con=engine, if_exists='append', index=False)\n",
    "\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    if j == 0:\n",
    "        return \"Não\"\n",
    "\n",
    "    else:\n",
    "        return \"Sim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37657e",
   "metadata": {
    "id": "5a37657e"
   },
   "source": [
    "#### <b>AGENTE 2: Extração e Aprendizado</b>\n",
    "<b>Responsabilidade:</b> Processar documentos e extrair dados relevantes<br/><br/>\n",
    "<b>Funcionalidades:</b>\n",
    "<ul><li>OCR avançado para digitalização de documentos</li></ul>\n",
    "<ul><li>NLP para identificação e extração de campos específicos</li></ul>\n",
    "<ul><li>IA para adaptação a novos layouts</li></ul>\n",
    "<ul><li>Validação cruzada de dados extraídos</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bb873",
   "metadata": {
    "id": "459bb873"
   },
   "outputs": [],
   "source": [
    "def agente2(pergunta,llm,engine):\n",
    "\n",
    "    print('\\nExecutando agente 2...')\n",
    "\n",
    "    # FORMATANDO A SAÍDA DA LLM COM JsonOutputParser\n",
    "    class Query(BaseModel):\n",
    "        query: str = Field(description='Esta é a query com DISTINCT e o nome de cada coluna entre \"')\n",
    "\n",
    "    parseador = JsonOutputParser(pydantic_object=Query)\n",
    "\n",
    "    # CRIANDO O PROMPT PARA A LLM COM A SAIDA FORMATADA\n",
    "    template_query = \"\"\"Qual query deve ser executada na tabela NFCABECALHO com as colunas {colunas_tab_cabecalho} ou tabela NFITENS com as colunas {colunas_tab_itens} para responder\n",
    "    a pergunta {pergunta}? Se a query envolver as duas tabelas, deve ser feito um JOIN entre elas utlizando a coluna \"CHAVE DE ACESSO\" como chave. {formatacao_saida}\"\"\"\n",
    "\n",
    "    prompt_template_query = PromptTemplate(\n",
    "                                            template=template_query,\n",
    "                                            input_variables=[\"pergunta\",\"colunas_tab_cabecalho\",\"colunas_tab_itens\"],\n",
    "                                            partial_variables={\"formatacao_saida\" : parseador.get_format_instructions()}\n",
    "                                          )\n",
    "\n",
    "    # CRIANDO A CADEIA DE EXECUÇÃO PARA A LLM\n",
    "    chain = prompt_template_query | llm | parseador\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        query1 = sqlalc.text('PRAGMA table_info(NFCABECALHO)')\n",
    "        rs = con.execute(query1)\n",
    "        rows = rs.fetchall()\n",
    "        colunas_query1 = sorted([col[1] for col in rows])\n",
    "        #print('Colunas query1: ', colunas_query1)\n",
    "\n",
    "        query2 = sqlalc.text('PRAGMA table_info(NFITENS)')\n",
    "        rs = con.execute(query2)\n",
    "        rows = rs.fetchall()\n",
    "        colunas_query2 = sorted([col[1] for col in rows])\n",
    "        #print('Colunas query2: ', colunas_query2)\n",
    "\n",
    "\n",
    "    query = chain.invoke(input={\"pergunta\":pergunta, \"colunas_tab_cabecalho\":colunas_query1,\"colunas_tab_itens\":colunas_query2})['query']\n",
    "\n",
    "    print('\\nQuery: ',query)\n",
    "\n",
    "    resposta = query\n",
    "\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a89a55",
   "metadata": {
    "id": "e5a89a55"
   },
   "source": [
    "#### <b>AGENTE 3: Resposta e Interação</b>\n",
    "<b>Responsabilidade:</b> Interface inteligente com usuários<br/><br/>\n",
    "<b>Funcionalidades:</b>\n",
    "<ul><li>Integração com LLMs para consultas em linguagem natural.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa552db",
   "metadata": {
    "id": "7fa552db"
   },
   "outputs": [],
   "source": [
    "def agente3(pergunta,arquivos):\n",
    "\n",
    "    if not exists('nfs_data.db'): # CRIAÇÃO DO BANCO DE DADOS PARA A PRIMEIRA EXECUÇÃO\n",
    "        print('\\nCriando o banco de dados nfs_data...')\n",
    "        DATABASE_URL = \"sqlite:///nfs_data.db\" # Define o nome do arquivo do banco de dados\n",
    "        engine = sqlalc.create_engine(DATABASE_URL)\n",
    "\n",
    "    else:\n",
    "        engine = sqlalc.create_engine(\"sqlite:///nfs_data.db\") # Conecta ao banco de dados existente\n",
    "\n",
    "\n",
    "    # INTEGRAÇÃO COM A LLM\n",
    "    load_dotenv() # CARREGANDO O ARQUIVO COM A API_KEY\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",  # ou \"gemini-2.0-pro\"\n",
    "        temperature=0.5,\n",
    "        google_api_key=getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "            print('\\nExecutando agente 3...')\n",
    "\n",
    "            print('\\nPergunta: ',pergunta)\n",
    "\n",
    "            resposta = agente1(pergunta,engine,arquivos,llm) # A ENGINE NÃO É FECHADA AUTOMATICAMENTE, APENAS AS CONEXÕES QUANDO USADAS COM WITH\n",
    "\n",
    "            if resposta == \"Sim\":\n",
    "                query = agente2(pergunta,llm,engine)\n",
    "\n",
    "                # # OBTENÇÃO DO RESULTADO DA QUERY\n",
    "                with engine.connect() as con:\n",
    "                        df = read_sql(query, con)\n",
    "                        resposta = df\n",
    "\n",
    "            elif resposta == \"Não\":\n",
    "                    raise SemResposta\n",
    "\n",
    "            # elif resposta == \"SemArquivoZip\":\n",
    "            #     raise SemArquivoZip\n",
    "\n",
    "            elif resposta == \"SemArquivoCabecalho\":\n",
    "                    raise SemArquivoCabecalho\n",
    "\n",
    "            elif resposta == \"SemArquivoItens\":\n",
    "                    raise SemArquivoItens\n",
    "\n",
    "\n",
    "    # EXECUÇÃO DAS EXCEÇÕES\n",
    "    except SemArquivoCabecalho:\n",
    "            resposta = \"SemArquivoCabecalho\"\n",
    "            print('\\nResposta: ', resposta)\n",
    "\n",
    "    except SemArquivoItens:\n",
    "            resposta = \"SemArquivoItens\"\n",
    "\n",
    "    # except SemArquivoZip:\n",
    "    #     caminho_absoluto = abspath(diretorio)\n",
    "    #     print(f'\\nNão há arquivos zipados no diretório {caminho_absoluto}!\\n')\n",
    "\n",
    "    except SemResposta:\n",
    "            resposta = \"SemResposta\"\n",
    "\n",
    "    print('\\nResposta\\n',f'{resposta}')\n",
    "    \n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b31d0c",
   "metadata": {
    "id": "76b31d0c"
   },
   "source": [
    "#### <b>TESTANDO</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e76b144",
   "metadata": {
    "id": "9e76b144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executando agente 3...\n",
      "\n",
      "Pergunta:  Qual é a descrição dos serviços e a natureza da operação da nf com número 2525 ?\n",
      "\n",
      "Executando agente 1...\n",
      "Arquivos descompactados: ['202401_NFs_Cabecalho.csv', '202401_NFs_Itens.csv']\n",
      "Sim para o arquivo:  202401_NFs_Cabecalho.csv\n",
      "Sim para o arquivo:  202401_NFs_Itens.csv\n",
      "\n",
      "Executando agente 2...\n",
      "\n",
      "Query:  SELECT DISTINCT \"DESCRIÇÃO DO PRODUTO/SERVIÇO\", \"NATUREZA DA OPERAÇÃO\" FROM NFITENS WHERE \"NÚMERO\" = '2525'\n",
      "\n",
      "Resposta: \n",
      "    DESCRIÇÃO DO PRODUTO/SERVIÇO                NATUREZA DA OPERAÇÃO\n",
      "0  LANTERNA TATERAL CARRETA LED  VENDA DE MERCADORIA FORA DO ESTADO\n",
      "1            CINEMATICO RODO-AR  VENDA DE MERCADORIA FORA DO ESTADO\n",
      "2  ESPIRAL NYLON CABINE AMARELO  VENDA DE MERCADORIA FORA DO ESTADO\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "     #arquivo = \".\\\\202401_NFS - new.zip\"  # Diretório onde os arquivos zipados estão localizados\n",
    "     \n",
    "     arquivo = \"202401_NFS.zip\"  # Diretório onde os arquivos zipados estão localizados\n",
    "     \n",
    "#    # EXEMPLOS DE PERGUNTA PARA TESTE. ELAS DEVEM SER OBTIDAS DO FRONTEND\n",
    "     pergunta = \"Qual é a chave de acesso da nota 3510129 ?\"\n",
    "     pergunta = \"Quem descobriu o Brasil ?\"\n",
    "     pergunta = \"Qual é a descrição dos serviços de nf com número 2525 ?\"\n",
    "     pergunta = \"Qual é a descrição dos serviços e a natureza da operação da nf com número 2525 ?\"\n",
    "\n",
    "     resposta = agente3(pergunta, arquivo)  # Chama a função principal com a pergunta e o diretório\n",
    "     print('\\nResposta: \\n',resposta)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agente_nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
